# Quick Reference: Breaking Changes

A condensed reference for all breaking changes. Use this to quickly explain issues to users.

---

## ğŸ”´ HIGH Severity (Immediate Failures)

### BC-17.3-001: input_file_name() Removed
âŒ `df.withColumn("src", input_file_name())`  
ğŸ” `\binput_file_name\s*\(`  
âœ… `df.withColumn("src", col("_metadata.file_name"))`  
âœ… SQL: `SELECT _metadata.file_name FROM ...`

### BC-13.3-001: MERGE INTO Type Casting (ANSI Mode)
âŒ `MERGE INTO target SET int_col = bigint_col` *(overflow throws error)*  
ğŸ” `\bMERGE\s+INTO\b`  
âœ… Add explicit bounds checking: `CASE WHEN val > 2147483647 THEN NULL ELSE CAST(val AS INT) END`

### BC-16.4-001a: Scala JavaConverters
âŒ `import scala.collection.JavaConverters._`  
ğŸ” `import\s+scala\.collection\.JavaConverters`  
âœ… `import scala.jdk.CollectionConverters._`

### BC-16.4-001c: Scala TraversableOnce
âŒ `def process(items: TraversableOnce[T])`  
ğŸ” `\bTraversableOnce\b`  
âœ… `def process(items: IterableOnce[T])`

### BC-16.4-001d: Scala Traversable
âŒ `def process(data: Traversable[Int])`  
ğŸ” `\bTraversable\b(?!Once)`  
âœ… `def process(data: Iterable[Int])`

### BC-16.4-002: HashMap/HashSet Ordering Changed
âŒ `HashMap("a" -> 1, "b" -> 2).foreach(...)` *(order may differ)*  
ğŸ” `\b(HashMap|HashSet)\s*[\[\(]`  
âœ… `map.toSeq.sortBy(_._1).foreach(...)` or use `ListMap`

### BC-SC-001: Spark Connect Lazy Analysis
âŒ `try: df.withColumn("x", col("bad_col"))` *(error at action, not transform)*  
ğŸ” `try\s*:` near DataFrame transforms  
âœ… Add `_ = df.columns` after transform to force early validation

---

## ğŸŸ¡ MEDIUM Severity (Potential Issues)

### BC-15.4-003: '!' Syntax for NOT
âŒ `CREATE TABLE IF ! EXISTS t`  
âŒ `WHERE status IS ! NULL`  
âŒ `WHERE id ! IN (1,2,3)`  
ğŸ” `(IF|IS)\s*!(?!\s*=)` and `\s!\s*(IN|BETWEEN|LIKE|EXISTS)`  
âœ… `IF NOT EXISTS`, `IS NOT NULL`, `NOT IN`

### BC-16.4-001b: Scala .to[Collection]
âŒ `list.to[List]`  
ğŸ” `\.to\s*\[\s*(List|Set|Vector|Seq|Array)\s*\]`  
âœ… `list.to(List)`

### BC-16.4-001e: Scala Stream
âŒ `Stream.from(1)`  
ğŸ” `\bStream\s*\.\s*(from|continually|iterate)`  
âœ… `LazyList.from(1)`

### BC-16.4-001f: Scala .toIterator
âŒ `list.toIterator`  
ğŸ” `\.toIterator\b`  
âœ… `list.iterator`

### BC-16.4-001g: Scala .view.force
âŒ `list.view.map(_ * 2).force`  
ğŸ” `\.view\s*\.\s*force\b`  
âœ… `list.view.map(_ * 2).to(List)`

### BC-16.4-001h: Scala collection.Seq Changed
âŒ `import scala.collection.Seq` *(now immutable)*  
ğŸ” `\bcollection\.Seq\b`  
âœ… Use explicit `immutable.Seq` or `mutable.Seq`

### BC-13.3-003: overwriteSchema + Dynamic Partition
âŒ Using both `overwriteSchema=true` and `partitionOverwriteMode=dynamic`  
ğŸ” `overwriteSchema.*true` near partition operations  
âœ… Separate into two operations: schema evolution first, then partition overwrite

### BC-17.3-002: Auto Loader Default Changed
âŒ Implicit `cloudFiles.useIncrementalListing` behavior  
ğŸ” `format\s*\(\s*[\"']cloudFiles[\"']\s*\)`  
âœ… Set explicitly: `.option("cloudFiles.useIncrementalListing", "auto")`

### BC-15.4-006: VIEW Schema Binding Mode
âŒ View schema binding mode changed  
ğŸ” `CREATE\s+(OR\s+REPLACE\s+)?VIEW`  
âœ… Review schema evolution behavior on target DBR

### BC-16.4-003: Data Source Cache Options
âŒ Cached reads may ignore options  
ğŸ” `spark\.sql\.legacy\.readFileSourceTableCacheIgnoreOptions`  
âœ… Set `spark.sql.legacy.readFileSourceTableCacheIgnoreOptions = true`

### BC-16.4-006: Auto Loader cleanSource Behavior
âŒ cleanSource file deletion timing changed  
ğŸ” `cloudFiles\.cleanSource`  
âœ… Review file cleanup behavior and timing

---

## ğŸŸ¢ LOW Severity (Subtle Changes)

### BC-15.4-001: VARIANT in Python UDF [REVIEW]
âš ï¸ `@udf(returnType=VariantType())` *(may fail in 15.4+)*  
ğŸ” `VariantType\s*\(`  
âœ… Test on target DBR or use `StringType()` + `json.dumps()`, then `parse_json()` later

### BC-15.4-004: View Column Types
âŒ `CREATE VIEW v (id INT, name STRING) AS SELECT ...`  
ğŸ” `CREATE\s+VIEW.*\([^)]*\b(INT|STRING|BIGINT)\b`  
âœ… Use `CAST()` in the SELECT instead

### BC-13.3-002: Parquet Timestamp NTZ
ğŸ” `spark\.sql\.parquet\.inferTimestampNTZ`  
âœ… Set `spark.sql.parquet.inferTimestampNTZ.enabled = false` for old behavior

### BC-13.3-004: ANSI Store Assignment Policy
ğŸ” `spark\.sql\.storeAssignmentPolicy`  
âœ… Review type assignment behavior in MERGE/UPDATE

### BC-15.4-002: JDBC Null Calendar
ğŸ” `spark\.sql\.legacy\.jdbc\.useNullCalendar`  
âœ… Set `spark.sql.legacy.jdbc.useNullCalendar = false` for old behavior

### BC-15.4-005: JDBC Reads
ğŸ” `\.jdbc\(|\.format\s*\(\s*[\"']jdbc[\"']\s*\)`  
âœ… Run self-comparison test (read with `useNullCalendar=true` vs `false`, diff with `exceptAll`). Fix only if diff > 0

### BC-16.4-004: MERGE materializeSource
ğŸ” `merge\.materializeSource.*none`  
âœ… Remove setting or use `"auto"`

### BC-16.4-001i: Scala Symbol Literals
âŒ `val sym = 'mySymbol`  
ğŸ” `'[a-zA-Z_][a-zA-Z0-9_]*`  
âœ… `val sym = Symbol("mySymbol")`

### BC-16.4-005: Json4s Library
ğŸ” `import\s+org\.json4s`  
âœ… Review json4s usage for compatibility

### BC-17.3-003: Spark Connect Null Handling
âŒ `array(lit(null))` *(may behave differently in Connect)*  
ğŸ” `(array|map|struct)\s*\(`  
âœ… Handle null values explicitly

### BC-17.3-004: Spark Connect Decimal Precision
âŒ `DecimalType()` without precision  
ğŸ” `DecimalType\s*\(`  
âœ… Specify precision and scale explicitly

### BC-14.3-001: Thriftserver hive.aux.jars.path
ğŸ” `hive\.aux\.jars\.path`  
âœ… Config removed - use alternative approach

### BC-SC-003: UDF Variable Capture [MANUAL REVIEW]
âŒ UDF captures external variable that changes later  
ğŸ” `@udf\s*\(`  
âœ… Use function factory pattern to capture at definition time

### BC-SC-004: Schema in Loops [MANUAL REVIEW]
âŒ `for col in df.columns:` (RPC on each iteration in Connect)  
ğŸ” `\.(columns|schema|dtypes)\b`  
âœ… Cache first: `cols = df.columns; for col in cols:`

---

## Auto-Fix Summary

| ID | Auto-Fixed | Notes |
|----|------------|-------|
| BC-17.3-001 | âœ… | DataFrame API & SQL strings |
| BC-15.4-003 | âœ… | All `!` â†’ `NOT` |
| BC-16.4-001a | âœ… | JavaConverters â†’ CollectionConverters |
| BC-16.4-001b | âœ… | `.to[List]` â†’ `.to(List)` |
| BC-16.4-001c | âœ… | TraversableOnce â†’ IterableOnce |
| BC-16.4-001d | âœ… | Traversable â†’ Iterable |
| BC-16.4-001e | âœ… | Stream â†’ LazyList |
| BC-16.4-001f | âœ… | `.toIterator` â†’ `.iterator` |
| BC-16.4-001g | âœ… | `.view.force` â†’ `.view.to(List)` |
| BC-16.4-001i | âœ… | `'symbol` â†’ `Symbol("symbol")` |
| BC-13.3-001 | âŒ | Manual - review type casting |
| BC-15.4-001 | âŒ | Manual - test or rewrite |
| BC-17.3-002 | âŒ | Config - test first |
| BC-SC-* | âŒ | Manual review required |

---

## Pattern Counts by Category

| Category | Count | IDs |
|----------|-------|-----|
| ğŸ”´ Auto-Fix | 10 | BC-17.3-001, BC-15.4-003, BC-16.4-001a-i |
| ğŸŸ  Assisted Fix | 11 | BC-SC-002/003, BC-17.3-005, BC-13.3-002/004, BC-15.4-002/005, BC-16.4-003/004/006, BC-17.3-002 |
| ğŸŸ¡ Manual Review | 10 | BC-13.3-001/003, BC-15.4-001/004/006, BC-16.4-002, BC-SC-001/004, BC-17.3-003/004 |
| **Total** | **31** | All patterns |
